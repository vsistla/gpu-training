{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is used as a demo file for Global AI/Big Data Conference presentation - on Sept 16th 2020 - to discuss some of the aspects of GPU training of NLP models using PyTorch.\n",
    "\n",
    "# Agenda\n",
    "\n",
    "> 1. Background & Expectations\n",
    "> 2. Quick overview of the GPU platform I am using for the demo - trainml.ai\n",
    "> 3. Hyperparameters & Training\n",
    "> 4. Options to check while the model is training\n",
    "> 5. Memory management\n",
    "> 6. Some experimental data\n",
    "> 7. Final Thoughts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Background & Expectations\n",
    "> - This is the original file from minGPT - but have added sections to specifically discuss about GPU training aspects and some of the challenges. gptMin code base is used as back drop in doing this demo. This is the original repo - https://github.com/karpathy/minGPT\n",
    "\n",
    "> - So, I will use this code base to \"train\" a min GPT model BUT please note that I will not able to complete the training - as each such training will take upwards of 15 mins and I only 30 mins to do this demo. So, I will keep starting and stopping the training to demonstrates aspect you all need to be aware of when doing GPU training. \n",
    "\n",
    "> - Also, I only discuss play_char.ipynb - which trains a GPT to be a character-level language model on arbitrary text, similar to Karpathy's older char-rnn but with a transformer instead of an RNN\n",
    "\n",
    "> - Since minGPT uses PyTorch, all my commands and examples are limited to PyTorch and may or may not apply to other libraries such as TensorFlow, Kera, etc. \n",
    "\n",
    "> - Finally all this is available in a git repo I setup - I will post the link to the repo at the end of the demo but here's is the link - https://github.com/vsistla/gpu-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Overview of GPU Platform - What to look for when choosing/picking a platform. \n",
    "### I will share more about the platform at the end of this demo and also I have surprise promotion - that will only work after we complete the demo. \n",
    "> - Ease of use\n",
    "> - Transparent pricing\n",
    "> - Cheaper for experimentation\n",
    "> - Ideal for starting small and progressively increasing your infrastructure footprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "\n",
    "## Tip - The logging module uses handlers attached to loggers to decide how, where, or even if messages ultimately get \n",
    "## stored or displayed. You can configure logging by default to write to a file as well.\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip - Language model will actually make block_size individual predictions at the same time based on this data\n",
    "# This is used in TrainerConfig, GPTConfig & CharDataset as well\n",
    "    \n",
    "block_size = 128 # 128 spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-12 23:14:38--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.192.133, 151.101.128.133, 151.101.0.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2020-09-12 23:14:38 (15.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Hyperparameters & Training\n",
    "> This is a quick overview of only some of the hyperparameters that will directly or indirectly impact your training duration, GPU memory utilization, etc. \n",
    "\n",
    "### 1. Epochs - \n",
    "> Usually the training takes several data passes (epochs) and the number of epochs has to be adjusted: too few can lead to underfit, and too many to overfit. Of course, this is complexity-dependant, but once we have our network configured, we can keep track on both training and testing datasets of the loss along the epochs in order to get an idea of when to stop training.\n",
    "\n",
    "### 2. batch size - \n",
    "> one of the parameters that has bearing on amount of memory being utilized\n",
    "\n",
    "### 3. number of layers - 8 and number of heads is 8 and embeddings - word/token is 512. \n",
    "> As compared to say \"\"\" GPT-1 like network roughly 125M params \"\"\"\n",
    "    n_layer = 12\n",
    "    n_head = 12\n",
    "    n_embd = 768\n",
    "### 4. Number of Features/attributes of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/16/2020 14:49:20 - INFO - mingpt.model -   number of parameters: 2.535219e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximately, 25 Million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2179 [00:00<?, ?it/s]/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "epoch 1 iter 2178: train loss 0.38387. lr 3.000169e-04: 100%|██████████| 2179/2179 [15:10<00:00,  2.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=1, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Options to check while the model is training\n",
    "> - <code>Top</code> and <code> Htop </code>\n",
    "\n",
    "> For example, in terminal you can try <code> top </code> or <code> htop</code>\n",
    "\n",
    "> Note about <code> top </code> - By default, top displays CPU usage as a percentage of a single CPU. On multi-core systems, you can see percentages of CPU usage are greater than 100%. You can toggle this behavior by hitting Shift + i while top is running to show the overall percentage of available CPUs in use.\n",
    "\n",
    "> To install <code> htop </code>, run <code> apt-get install htop </code>\n",
    "\n",
    "> - <code> nvidia-smi</code> and <code> nvidia-smi -a</code> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Memory Management\n",
    "## Few points to note\n",
    "> - Understanding of the health and status of GPU infrastructure has always been very difficult. Also difficult to determine the type and capabilities of the GPUs in a system. So, start with something simple and scale. Crawl before you walk. \n",
    "\n",
    "> - This is the most important aspect of GPU training. This is a very involved topic - lot more than what we can cover in 1/2 hr session\n",
    "> - Each GPU vendor provides their own commandline utilities and each of the libraries such a PyTorch,Keras, Tensorflow provide their own packages to help manage your memory footprint. This is where you can get the maximum bang for your nuck\n",
    "\n",
    "### 1. torch methods - torch.cuda.empty_cache()\n",
    "#### - torch.cuda.empty_cache()\n",
    "> - Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible in nvidia-smi.\n",
    "\n",
    "### PyTorch uses a caching memory allocator to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in nvidia-smi. \n",
    "\n",
    "#### - torch.cuda.memory_stats(#device_number)\n",
    "> - Returns a dictionary of CUDA memory allocator statistics for a given device.\n",
    "The return value of this function is a dictionary of statistics\n",
    "\n",
    "#### - memory_allocated, memory_cached, max_memory\n",
    "\n",
    "> -  You can use memory_allocated() and max_memory_allocated() to monitor memory occupied by tensors, and use memory_cached() and max_memory_cached() to monitor memory managed by the caching allocator. \n",
    "\n",
    "> -  Calling empty_cache() releases all unused cached memory from PyTorch so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch.\n",
    "\n",
    "> But note that these utilities don’t really you give you the complete memory utilization - for example, this doesn’t take into account of cuda related memory usage - cuda ctx, cuda rng state, cudnn ctx, cufft plans, and other gpu memory your other libraries\n",
    "\n",
    "\n",
    "## Out of memory (CUDA OOM, etc)\n",
    "> - When running multiple processes - GPU memory is blocked by the initial process and the next process that is running doens’t have access to that memory block …... Easiest way to deal with this is to “shutdown the previous kernel” using your Jupyter notebook interface and restart the new kernel/process. But this is not a scalable option\n",
    "\n",
    "> - The GPU, on the other hand, doesn’t release its memory after model training. So training specific variables, such as the momentum values for the Adam optimizer (two variables for every model parameter), stay in memory. As a result, there’s much less memory left for evaluation. To fix the out-of-memory error, we use a smaller validation batch size\n",
    "\n",
    "> - TPU (in colab) Works differently - The reason is that the TPU is in a pod somewhere else, unlike the GPU, which is inside Colab’s machine or your TPU cluster. So, to use the TPU, we have to access it through the network. After training our model, the TPU is disconnected and closed, and its memory is cleared. When we evaluate the model, we re-connect to the TPU, which has a clean memory, which is sufficient to hold the entire validation set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.memory_stats(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6952058880"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_cached(0)\n",
    "# 2 GPUs max_memory_cached(0) is 7637827584\n",
    "# div by 10e9 to GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6806414848"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated(0)\n",
    "# 2 GPUs max_memory_allocated(0) = 7621877760"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(0)\n",
    "# 2 GPUs cuda.memory_allocated(0)= 417400320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached(0)\n",
    "# 2 GPUs cuda.memory_cached(0) = 7054819328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cached minus allocated free memory\n",
    "torch.cuda.memory_cached(0) - torch.cuda.memory_allocated(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tip - This script looks up memory usage across all the gpu machines\n",
    "# courtesy of mjstevens777 Matt\n",
    "# In here, we are nvidia-smi programmatically ....\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def get_gpu_memory_map():\n",
    "    \"\"\"Get the current gpu usage.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    usage: dict\n",
    "        Keys are device ids as integers.\n",
    "        Values are memory usage as integers in MB.\n",
    "    \"\"\"\n",
    "    result = subprocess.check_output(\n",
    "        [\n",
    "            'nvidia-smi', '--query-gpu=memory.used',\n",
    "            '--format=csv,nounits,noheader'\n",
    "        ], encoding='utf-8')\n",
    "    # Convert lines into a dictionary\n",
    "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
    "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
    "    return gpu_memory_map\n",
    "get_gpu_memory_map()\n",
    "#{0: 7326, 1: 6992, 2: 6992, 3: 6992}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (5.7.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import gc\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gc.collect returns number of unreachable objects. \n",
    "# It's really the sum of two numbers: the number of objects that were identified as \n",
    "# garbage and actually freed, plus the number of objects that were identified as garbage but could not be freed.\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 11554717696\n",
      "free     : 11542855680\n",
      "used     : 11862016\n"
     ]
    }
   ],
   "source": [
    "# make sure you run this pip install pynvml\n",
    "\n",
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')\n",
    "\n",
    "# total    : 8370061312\n",
    "# free     : 687865856\n",
    "# used     : 7682195456\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache() will release all the GPU memory cache that can be freed.....can be freed ....\n",
    "# So, the variables no longer referenced will be freed by using torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "Building wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=f1e403e747190845709b0aa05d81daef0f605459cf32dd7dbc5fb13a8b980057\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil\n",
      "Successfully installed GPUtil-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run nvidia-smi\n",
    "# run nvidia-smi -a - for detailed log of all the stats\n",
    "\n",
    "# If you want to programmatically pull the data that you see in nvidia-smi, here's one way of doing that\n",
    "\n",
    "# make sure to install GPUtil before running this utility\n",
    "\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "print(\"GPU Usage\")\n",
    "gpu_usage(all=True)                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Usage after emptying the cache\")\n",
    "torch.cuda.empty_cache()\n",
    "gpu_usage(all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache() deletes unused tensor from the cache, but the cache itself still uses some memory.\n",
    "# PyTorch's caching allocator reserves some fixed amount of memory even if there are no tensors, \n",
    "# and this allocation is triggered by the first CUDA memory access \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to run this in terminal\n",
    "!watch -n 1 free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to check your GPU device names\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tracking Memory Usage with GPUtil\n",
    "\n",
    "\n",
    "!pip install GPUtil\n",
    "import GPUtil\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Some experimental data when trying different number of GPU machines and batch sizes\n",
    "\n",
    "## GPU Type and number of instances - \n",
    "> -  'GeForce RTX 2080 Ti' - 1 instance - batch size 512\n",
    "> -  CUDA OOM; Tried to allocate 256.00 MiB (GPU 0; 10.76 GiB total capacity; 9.76 GiB already allocated; 135.12 MiB free; 9.87 GiB reserved in total by PyTorch)\n",
    "\n",
    "> -  'GeForce RTX 2080 Ti' - 1 instance - batch size 256\n",
    "> -  CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.76 GiB total capacity; 9.96 GiB already allocated; 5.12 MiB free; 9.99 GiB reserved in total by PyTorch)\n",
    "\n",
    "> - 'GeForce RTX 2080 Ti' - 1 instance - batch size 128\n",
    "> -  CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 9.99 GiB already allocated; 5.12 MiB free; 9.99 GiB reserved in total by PyTorch)\n",
    "\n",
    "> -  'GeForce RTX 2080 Ti' - 1 instance - batch size 64\n",
    "> - Runs\n",
    "\n",
    "> -  'GeForce RTX 2080 Ti' - 2 instance - batch size 128\n",
    "> -  Runs\n",
    "\n",
    "> -  'GeForce RTX 2080 Ti' - 2 instance - batch size 256\n",
    "> -  CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.76 GiB total capacity; 9.89 GiB already allocated; 95.12 MiB free; 9.90 GiB reserved in total by PyTorch)\n",
    "\n",
    "> -  'GeForce RTX 2080 Ti' - 4 instances - batch size 256 - 1 epochs - 18 mins\n",
    "> -  Runs Train loss - 0.23094\n",
    " \n",
    "\n",
    "> - 'GeForce RTX 2080 Ti' - 4 instances - batch size 512 - 1 epochs - 18 mins\n",
    "> -   Runs Train loss - 0.14909\n",
    " \n",
    "\n",
    "> -  'GeForce RTX 2080 Ti' - 4 instances - batch size 1024\n",
    "> -  CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.76 GiB total capacity; 9.79 GiB already allocated; 127.12 MiB free; 9.87 GiB reserved in total by PyTorch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! that e'er this tongue of mine,\n",
      "That laid the sentence of dread banishment\n",
      "On yon proud man, should take it off again\n",
      "With words of sooth! O that I were as good\n",
      "What I have said, Bianca, get you in:\n",
      "And let it not displease thee, good Bianca,\n",
      "For I will love thee ne'er the less, my girl.\n",
      "\n",
      "KATHARINA:\n",
      "A pretty peat! it is best\n",
      "Put finger in the eye, an she knew why.\n",
      "\n",
      "BIANCA:\n",
      "Sister, content you in my discontent.\n",
      "Sir, to your pleasure humbly I subscribe:\n",
      "My books and instruments shall be my company,\n",
      "On them to took a town of mine eyes:\n",
      "Sleeping and waking, O, defend me still!\n",
      "\n",
      "Ghost of Prince Edward:\n",
      "\n",
      "Ghost of King Henry VI:\n",
      "\n",
      "Ghost of CLARENCE:\n",
      "\n",
      "Ghost of RIVERS:\n",
      "\n",
      "Ghost of GREY:\n",
      "\n",
      "Ghost of VAUGHAN:\n",
      "\n",
      "All:\n",
      "\n",
      "Ghost of HASTINGS:\n",
      "\n",
      "Ghosts of young Princes:\n",
      "\n",
      "Ghost of LADY ANNE:\n",
      "\n",
      "Ghost of BUCKINGHAM:\n",
      "\n",
      "KING RICHARD III:\n",
      "Give me another horse: bind up my wounds.\n",
      "Have mercy, Jesu!--Soft! I did but dream.\n",
      "O coward conscience, how dost thou afflict me!\n",
      "The lights burn blue. It is now dead midnight.\n",
      "Cold fearful drops stand on my trembling flesh.\n",
      "What do I fear?  myself?   there's none else by:\n",
      "Richard loves Richard; that is, I am I.\n",
      "Is there a murderer here?    No. Yes, I am:\n",
      "Then fly. What, from myself?   Great reason why:\n",
      "Lest I revenge. What, myself upon myself?\n",
      "Alack. I love myself. Wherefore? for any good\n",
      "That I myself have done unto myself?\n",
      "O, no! alas, I rather hate myself\n",
      "For hateful deeds committed by myself!\n",
      "I am a villain: yet I lie. I am not.\n",
      "Fool, of thyself speak well: fool, do not flatter.\n",
      "My conscience hath a thousand several tongues,\n",
      "And every tongue brings in a several tale,\n",
      "And every tale condemns me for a villain.\n",
      "Perjury, perjury, in the high'st degree\n",
      "Murder, stem murder, in the direst degree;\n",
      "All several sins, all used in each degree,\n",
      "Throng to the bar, crying all, Guilty! guilty!\n",
      "I shall despair. There is no creature loves me;\n",
      "And if I die, no soul shall pity me:\n",
      "Nay, wherefore should they, since that I myself\n",
      "Find in myself no pity to myself?\n",
      "Methought the sou\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level Shakespeare\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"O God, O God!\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By @smth \n",
    "# If you also want to check cpuStats while do your GPU training, you can use psutil \n",
    "\n",
    "# prints currently alive Tensors and Variables\n",
    "# courtesy Smth, PyTorch Dev, Facebook AI Research\n",
    "# use the garbage collector’s book-keeping \n",
    "# to print out the currently resident Tensors. \n",
    "# Here’s a snippet that shows all the currently allocated Tensors:\n",
    "\n",
    "import sys, os, gc\n",
    "import psutil\n",
    "def memReport():\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nmemReport\\n\")\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(\"sys.version: \", sys.version)\n",
    "        #  cpu_percent() keeps track of CPU times since last call, and that's how it is able to determine percentage.\n",
    "        print(\"\\nCPU percent: \", psutil.cpu_percent())\n",
    "        print(\"\\nPhysical memory: \", psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid() # this returns the current process\n",
    "        print(\"\\npid: \", pid)\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('\\nmemory GB:', memoryUse)\n",
    "\n",
    "memReport()\n",
    "cpuStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Final Comments \n",
    "\n",
    "### Memory management - \n",
    "\n",
    "> - Unlike many other languages, Python does not necessarily release the memory back to the Operating System. Instead, it has a dedicated object allocator for smaller objects.\n",
    "\n",
    "\n",
    "### Python garbage collection \n",
    "\n",
    "> - can also come in handy - but most of the time you don't need to use gc.collect but there some advantages in calling gs.collect at the end of your loop - you will reduce memory fragmentation. Trying to free the memory between computations using gc.collect() fixed the memory-related aspect of the problem but it could also result in performance issues.\n",
    "\n",
    "### Multi threaded programming\n",
    "\n",
    "> - Of course Multi threaded programming might work much better instead of having to do gc.collect after the end of every loop. Once the thread ends, the memory is automatically freed without the strange performance issue. \n",
    "\n",
    "\n",
    "### CPU Utilization (in Top and CPUStat snippet) \n",
    "\n",
    "> - Remember that CPU percentage can jump around quite a lot from one moment to another and especially where the time period it is calculated over keeps changing. This can be quite confusing. Use a background thread which works out CPU percentage average over set time ranges.\n",
    "\n",
    "### __pycache__ and .pyc/.pyo files\n",
    "\n",
    "> - You will notice that Python created folders with __pycache__ and also files with .pyc or .pyo extension. These are bytecode-compiled and optimized bytecode-compiled versions of your program's files, respectively. They make your program start a little faster. When your scripts change, they will be recompiled, and if you delete the files or the whole folder and run your program again, they will reappear (unless you specifically suppress that behavior). Just put them in your gitignore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
