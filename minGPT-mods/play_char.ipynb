{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the original file from minGPT - but have added sections to specifically discuss about GPU training aspects and some of the challenges. gptMin code base is used as back drop in doing this demo\n",
    "\n",
    "## This is the original repo - https://github.com/karpathy/minGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a character-level GPT on some text data\n",
    "\n",
    "The inputs here are simple text files, which we chop up to individual characters and then train GPT on. So you could say this is a char-transformer instead of a char-rnn. Doesn't quite roll off the tongue as well. In this example we will feed it some Shakespeare, which we'll get it to predict character-level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make deterministic\n",
    "from mingpt.utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "\n",
    "    def __init__(self, data, block_size):\n",
    "        chars = sorted(list(set(data)))\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "        \n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "        self.block_size = block_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        chunk = self.data[idx:idx + self.block_size + 1]\n",
    "        # encode every character to an integer\n",
    "        dix = [self.stoi[s] for s in chunk]\n",
    "        \"\"\"\n",
    "        arrange data and targets so that the first i elements of x\n",
    "        will be asked to predict the i-th element of y. Notice that\n",
    "        the eventual language model will actually make block_size\n",
    "        individual predictions at the same time based on this data,\n",
    "        so we are being clever and amortizing the cost of the forward\n",
    "        pass of the network. So for example if block_size is 4, then\n",
    "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
    "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
    "        then actually \"multitask\" 4 separate examples at the same time\n",
    "        in the language model:\n",
    "        - given just \"h\", please predict \"e\" as next\n",
    "        - given \"he\" please predict \"l\" next\n",
    "        - given \"hel\" predict \"l\" next\n",
    "        - given \"hell\" predict \"o\" next\n",
    "        \n",
    "        In addition, because the DataLoader will create batches of examples,\n",
    "        every forward/backward pass during traning will simultaneously train\n",
    "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
    "        for a batched input of integers X (B, T) where B is batch size and\n",
    "        T is block_size and Y (B, T), the network will during training be\n",
    "        simultaneously training to make B*T predictions, all at once! Of course,\n",
    "        at test time we can paralellize across batch B, but unlike during training\n",
    "        we cannot parallelize across the time dimension T - we have to run\n",
    "        a forward pass of the network to recover the next single character of the \n",
    "        sequence along each batch dimension, and repeatedly always feed in a next\n",
    "        character to get the next one.\n",
    "        \n",
    "        So yes there is a big asymmetry between train/test time of autoregressive\n",
    "        models. During training we can go B*T at a time with every forward pass,\n",
    "        but during test time we can only go B at a time, T times, with T forward \n",
    "        passes.\n",
    "        \"\"\"\n",
    "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
    "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language model will actually make block_size individual predictions at the same time based on this data\n",
    "    \n",
    "block_size = 128 # 128 spatial extent of the model for its context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-09-12 23:14:38--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.192.133, 151.101.128.133, 151.101.0.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.192.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: ‘input.txt’\n",
      "\n",
      "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.07s   \n",
      "\n",
      "2020-09-12 23:14:38 (15.7 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "# you can download this file at https://github.com/karpathy/char-rnn/blob/master/data/tinyshakespeare/input.txt\n",
    "text = open('input.txt', 'r').read() # don't worry we won't run out of file handles\n",
    "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "09/15/2020 23:46:51 - INFO - mingpt.model -   number of parameters: 2.535219e+07\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT, GPTConfig\n",
    "mconf = GPTConfig(train_dataset.vocab_size, train_dataset.block_size,\n",
    "                  n_layer=8, n_head=8, n_embd=512)\n",
    "model = GPT(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_cached(0)\n",
    "# 2 GPUs max_memory_cached(0) is 7637827584\n",
    "# div by 10e9 to GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_allocated(0)\n",
    "# 2 GPUs max_memory_allocated(0) = 7621877760"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(0)\n",
    "# 2 GPUs cuda.memory_allocated(0)= 417400320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached(0)\n",
    "# 2 GPUs cuda.memory_cached(0) = 7054819328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cached minus allocated free memory\n",
    "torch.cuda.memory_cached(0) - torch.cuda.memory_allocated(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 11, 1: 11, 2: 11, 3: 11}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This script looks up memory usage across all the gpu machines\n",
    "# courtesy of mjstevens777 Matt\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def get_gpu_memory_map():\n",
    "    \"\"\"Get the current gpu usage.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    usage: dict\n",
    "        Keys are device ids as integers.\n",
    "        Values are memory usage as integers in MB.\n",
    "    \"\"\"\n",
    "    result = subprocess.check_output(\n",
    "        [\n",
    "            'nvidia-smi', '--query-gpu=memory.used',\n",
    "            '--format=csv,nounits,noheader'\n",
    "        ], encoding='utf-8')\n",
    "    # Convert lines into a dictionary\n",
    "    gpu_memory = [int(x) for x in result.strip().split('\\n')]\n",
    "    gpu_memory_map = dict(zip(range(len(gpu_memory)), gpu_memory))\n",
    "    return gpu_memory_map\n",
    "get_gpu_memory_map()\n",
    "#{0: 7326, 1: 6992, 2: 6992, 3: 6992}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.7/site-packages (5.7.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install psutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import gc\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gc.collect returns number of unreachable objects. \n",
    "# It's really the sum of two numbers: the number of objects that were identified as \n",
    "# garbage and actually freed, plus the number of objects that were identified as garbage but could not be freed.\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total    : 11554717696\n",
      "free     : 11542855680\n",
      "used     : 11862016\n"
     ]
    }
   ],
   "source": [
    "# make sure you run this pip install pynvml\n",
    "\n",
    "from pynvml import *\n",
    "nvmlInit()\n",
    "h = nvmlDeviceGetHandleByIndex(0)\n",
    "info = nvmlDeviceGetMemoryInfo(h)\n",
    "print(f'total    : {info.total}')\n",
    "print(f'free     : {info.free}')\n",
    "print(f'used     : {info.used}')\n",
    "\n",
    "# total    : 8370061312\n",
    "# free     : 687865856\n",
    "# used     : 7682195456\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache() will release all the GPU memory cache that can be freed.....can be freed ....\n",
    "# So, the variables no longer referenced will be freed by using torch.cuda.empty_cache()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting GPUtil\n",
      "  Downloading GPUtil-1.4.0.tar.gz (5.5 kB)\n",
      "Building wheels for collected packages: GPUtil\n",
      "  Building wheel for GPUtil (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for GPUtil: filename=GPUtil-1.4.0-py3-none-any.whl size=7411 sha256=f1e403e747190845709b0aa05d81daef0f605459cf32dd7dbc5fb13a8b980057\n",
      "  Stored in directory: /root/.cache/pip/wheels/6e/f8/83/534c52482d6da64622ddbf72cd93c35d2ef2881b78fd08ff0c\n",
      "Successfully built GPUtil\n",
      "Installing collected packages: GPUtil\n",
      "Successfully installed GPUtil-1.4.0\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.3 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install GPUtil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run nvidia-smi\n",
    "# run nvidia-smi -a - for detailed log of all the stats\n",
    "\n",
    "# If you want to programmatically pull the data that you see in nvidia-smi, here's one way of doing that\n",
    "\n",
    "# make sure to install GPUtil before running this utility\n",
    "\n",
    "import torch\n",
    "from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "print(\"GPU Usage\")\n",
    "gpu_usage(all=True)                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Usage after emptying the cache\")\n",
    "torch.cuda.empty_cache()\n",
    "gpu_usage(all=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache() deletes unused tensor from the cache, but the cache itself still uses some memory.\n",
    "# PyTorch's caching allocator reserves some fixed amount of memory even if there are no tensors, \n",
    "# and this allocation is triggered by the first CUDA memory access \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# better to run this in terminal\n",
    "!watch -n 1 free -m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to check your GPU device names\n",
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tracking Memory Usage with GPUtil\n",
    "\n",
    "\n",
    "!pip install GPUtil\n",
    "import GPUtil\n",
    "GPUtil.showUtilization()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/2179 [00:00<?, ?it/s]\u001b[A\u001b[A/opt/conda/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "epoch 1 iter 0: train loss 0.14902. lr 5.999999e-04:   0%|          | 0/2179 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 0: train loss 0.14902. lr 5.999999e-04:   0%|          | 1/2179 [00:00<27:35,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 1: train loss 0.23168. lr 5.999997e-04:   0%|          | 1/2179 [00:01<27:35,  1.32it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 1: train loss 0.23168. lr 5.999997e-04:   0%|          | 2/2179 [00:01<23:18,  1.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 2: train loss 0.24189. lr 5.999994e-04:   0%|          | 2/2179 [00:01<23:18,  1.56it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 2: train loss 0.24189. lr 5.999994e-04:   0%|          | 3/2179 [00:01<20:30,  1.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 3: train loss 0.21861. lr 5.999988e-04:   0%|          | 3/2179 [00:01<20:30,  1.77it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 3: train loss 0.21861. lr 5.999988e-04:   0%|          | 4/2179 [00:01<18:27,  1.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 4: train loss 0.20649. lr 5.999982e-04:   0%|          | 4/2179 [00:02<18:27,  1.96it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 4: train loss 0.20649. lr 5.999982e-04:   0%|          | 5/2179 [00:02<17:00,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 5: train loss 0.20110. lr 5.999973e-04:   0%|          | 5/2179 [00:02<17:00,  2.13it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 5: train loss 0.20110. lr 5.999973e-04:   0%|          | 6/2179 [00:02<16:03,  2.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 6: train loss 0.20205. lr 5.999963e-04:   0%|          | 6/2179 [00:03<16:03,  2.26it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 6: train loss 0.20205. lr 5.999963e-04:   0%|          | 7/2179 [00:03<15:20,  2.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 7: train loss 0.19729. lr 5.999952e-04:   0%|          | 7/2179 [00:03<15:20,  2.36it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 7: train loss 0.19729. lr 5.999952e-04:   0%|          | 8/2179 [00:03<14:48,  2.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 8: train loss 0.19960. lr 5.999939e-04:   0%|          | 8/2179 [00:03<14:48,  2.44it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 8: train loss 0.19960. lr 5.999939e-04:   0%|          | 9/2179 [00:03<14:22,  2.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 9: train loss 0.20070. lr 5.999924e-04:   0%|          | 9/2179 [00:04<14:22,  2.52it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 9: train loss 0.20070. lr 5.999924e-04:   0%|          | 10/2179 [00:04<14:11,  2.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 10: train loss 0.19348. lr 5.999908e-04:   0%|          | 10/2179 [00:04<14:11,  2.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 10: train loss 0.19348. lr 5.999908e-04:   1%|          | 11/2179 [00:04<13:54,  2.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 11: train loss 0.19485. lr 5.999891e-04:   1%|          | 11/2179 [00:04<13:54,  2.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 11: train loss 0.19485. lr 5.999891e-04:   1%|          | 12/2179 [00:04<13:45,  2.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 12: train loss 0.19323. lr 5.999871e-04:   1%|          | 12/2179 [00:05<13:45,  2.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 12: train loss 0.19323. lr 5.999871e-04:   1%|          | 13/2179 [00:05<13:36,  2.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 13: train loss 0.19436. lr 5.999850e-04:   1%|          | 13/2179 [00:05<13:36,  2.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 13: train loss 0.19436. lr 5.999850e-04:   1%|          | 14/2179 [00:05<13:29,  2.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 14: train loss 0.18746. lr 5.999828e-04:   1%|          | 14/2179 [00:06<13:29,  2.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 14: train loss 0.18746. lr 5.999828e-04:   1%|          | 15/2179 [00:06<13:31,  2.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 15: train loss 0.19403. lr 5.999804e-04:   1%|          | 15/2179 [00:06<13:31,  2.67it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 15: train loss 0.19403. lr 5.999804e-04:   1%|          | 16/2179 [00:06<13:42,  2.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 16: train loss 0.18891. lr 5.999779e-04:   1%|          | 16/2179 [00:06<13:42,  2.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 16: train loss 0.18891. lr 5.999779e-04:   1%|          | 17/2179 [00:06<13:39,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 17: train loss 0.19101. lr 5.999752e-04:   1%|          | 17/2179 [00:07<13:39,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 17: train loss 0.19101. lr 5.999752e-04:   1%|          | 18/2179 [00:07<13:37,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 18: train loss 0.18837. lr 5.999723e-04:   1%|          | 18/2179 [00:07<13:37,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 18: train loss 0.18837. lr 5.999723e-04:   1%|          | 19/2179 [00:07<13:35,  2.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 19: train loss 0.18336. lr 5.999693e-04:   1%|          | 19/2179 [00:07<13:35,  2.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 19: train loss 0.18336. lr 5.999693e-04:   1%|          | 20/2179 [00:07<13:36,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 20: train loss 0.18743. lr 5.999661e-04:   1%|          | 20/2179 [00:08<13:36,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 20: train loss 0.18743. lr 5.999661e-04:   1%|          | 21/2179 [00:08<13:38,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 21: train loss 0.18628. lr 5.999628e-04:   1%|          | 21/2179 [00:08<13:38,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 21: train loss 0.18628. lr 5.999628e-04:   1%|          | 22/2179 [00:08<14:34,  2.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 22: train loss 0.18865. lr 5.999593e-04:   1%|          | 22/2179 [00:09<14:34,  2.47it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 22: train loss 0.18865. lr 5.999593e-04:   1%|          | 23/2179 [00:09<14:22,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 23: train loss 0.18765. lr 5.999557e-04:   1%|          | 23/2179 [00:09<14:22,  2.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 23: train loss 0.18765. lr 5.999557e-04:   1%|          | 24/2179 [00:09<14:13,  2.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 24: train loss 0.18277. lr 5.999519e-04:   1%|          | 24/2179 [00:09<14:13,  2.53it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 24: train loss 0.18277. lr 5.999519e-04:   1%|          | 25/2179 [00:09<13:55,  2.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 25: train loss 0.18299. lr 5.999479e-04:   1%|          | 25/2179 [00:10<13:55,  2.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 25: train loss 0.18299. lr 5.999479e-04:   1%|          | 26/2179 [00:10<13:50,  2.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 26: train loss 0.18561. lr 5.999438e-04:   1%|          | 26/2179 [00:10<13:50,  2.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 26: train loss 0.18561. lr 5.999438e-04:   1%|          | 27/2179 [00:10<13:49,  2.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 27: train loss 0.18631. lr 5.999395e-04:   1%|          | 27/2179 [00:11<13:49,  2.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 27: train loss 0.18631. lr 5.999395e-04:   1%|▏         | 28/2179 [00:11<13:44,  2.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 28: train loss 0.18288. lr 5.999351e-04:   1%|▏         | 28/2179 [00:11<13:44,  2.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 28: train loss 0.18288. lr 5.999351e-04:   1%|▏         | 29/2179 [00:11<13:38,  2.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 29: train loss 0.18533. lr 5.999305e-04:   1%|▏         | 29/2179 [00:11<13:38,  2.63it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 29: train loss 0.18533. lr 5.999305e-04:   1%|▏         | 30/2179 [00:11<13:33,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 30: train loss 0.18074. lr 5.999258e-04:   1%|▏         | 30/2179 [00:12<13:33,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 30: train loss 0.18074. lr 5.999258e-04:   1%|▏         | 31/2179 [00:12<13:32,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 31: train loss 0.18065. lr 5.999209e-04:   1%|▏         | 31/2179 [00:12<13:32,  2.64it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 31: train loss 0.18065. lr 5.999209e-04:   1%|▏         | 32/2179 [00:12<13:31,  2.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 32: train loss 0.18395. lr 5.999159e-04:   1%|▏         | 32/2179 [00:12<13:31,  2.65it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 32: train loss 0.18395. lr 5.999159e-04:   2%|▏         | 33/2179 [00:12<13:40,  2.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 33: train loss 0.18519. lr 5.999107e-04:   2%|▏         | 33/2179 [00:13<13:40,  2.62it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 33: train loss 0.18519. lr 5.999107e-04:   2%|▏         | 34/2179 [00:13<13:44,  2.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 34: train loss 0.18714. lr 5.999053e-04:   2%|▏         | 34/2179 [00:13<13:44,  2.60it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 34: train loss 0.18714. lr 5.999053e-04:   2%|▏         | 35/2179 [00:13<13:47,  2.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 35: train loss 0.18348. lr 5.998998e-04:   2%|▏         | 35/2179 [00:14<13:47,  2.59it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 35: train loss 0.18348. lr 5.998998e-04:   2%|▏         | 36/2179 [00:14<13:51,  2.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 36: train loss 0.18255. lr 5.998941e-04:   2%|▏         | 36/2179 [00:14<13:51,  2.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 36: train loss 0.18255. lr 5.998941e-04:   2%|▏         | 37/2179 [00:14<13:49,  2.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 37: train loss 0.17873. lr 5.998883e-04:   2%|▏         | 37/2179 [00:14<13:49,  2.58it/s]\u001b[A\u001b[A\n",
      "\n",
      "epoch 1 iter 37: train loss 0.17873. lr 5.998883e-04:   2%|▏         | 38/2179 [00:14<13:46,  2.59it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5f653bf61de3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                       num_workers=4)\n\u001b[1;32m      7\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/models/minGPT-mods/mingpt/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_dataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0mtest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/models/minGPT-mods/mingpt/trainer.py\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(split)\u001b[0m\n\u001b[1;32m     88\u001b[0m                     \u001b[0;31m# backprop and update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_norm_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer, TrainerConfig\n",
    "\n",
    "# initialize a trainer instance and kick off training\n",
    "tconf = TrainerConfig(max_epochs=1, batch_size=512, learning_rate=6e-4,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_dataset)*block_size,\n",
    "                      num_workers=4)\n",
    "trainer = Trainer(model, train_dataset, None, tconf)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Type and number of instances - \n",
    "### 'GeForce RTX 2080 Ti' - 1 instance - batch size 512\n",
    "#### CUDA OOM; Tried to allocate 256.00 MiB (GPU 0; 10.76 GiB total capacity; 9.76 GiB already allocated; 135.12 MiB free; 9.87 GiB reserved in total by PyTorch)\n",
    "\n",
    "### 'GeForce RTX 2080 Ti' - 1 instance - batch size 256\n",
    "#### CUDA out of memory. Tried to allocate 64.00 MiB (GPU 0; 10.76 GiB total capacity; 9.96 GiB already allocated; 5.12 MiB free; 9.99 GiB reserved in total by PyTorch)\n",
    "\n",
    "### 'GeForce RTX 2080 Ti' - 1 instance - batch size 128\n",
    "#### CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.76 GiB total capacity; 9.99 GiB already allocated; 5.12 MiB free; 9.99 GiB reserved in total by PyTorch)\n",
    "\n",
    "### 'GeForce RTX 2080 Ti' - 1 instance - batch size 64\n",
    "#### Runs\n",
    "\n",
    "### 'GeForce RTX 2080 Ti' - 2 instance - batch size 128\n",
    "#### Runs\n",
    "\n",
    "### 'GeForce RTX 2080 Ti' - 2 instance - batch size 256\n",
    "#### CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.76 GiB total capacity; 9.89 GiB already allocated; 95.12 MiB free; 9.90 GiB reserved in total by PyTorch)\n",
    "\n",
    "### 'GeForce RTX 2080 Ti' - 4 instances - batch size 256 - 1 epochs - 18 mins\n",
    "#### Runs Train loss - 0.23094\n",
    "#### \n",
    "\n",
    "### 'GeForce RTX 2080 Ti' - 4 instances - batch size 512 - 1 epochs - 18 mins\n",
    "#### Runs Train loss - 0.14909\n",
    "#### \n",
    "\n",
    "### 'GeForce RTX 2080 Ti' - 4 instances - batch size 1024\n",
    "#### CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 10.76 GiB total capacity; 9.79 GiB already allocated; 127.12 MiB free; 9.87 GiB reserved in total by PyTorch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O God, O God! that e'er this tongue of mine,\n",
      "That laid the sentence of dread banishment\n",
      "On yon proud man, should take it off again\n",
      "With words of sooth! O that I were as good\n",
      "What I have said, Bianca, get you in:\n",
      "And let it not displease thee, good Bianca,\n",
      "For I will love thee ne'er the less, my girl.\n",
      "\n",
      "KATHARINA:\n",
      "A pretty peat! it is best\n",
      "Put finger in the eye, an she knew why.\n",
      "\n",
      "BIANCA:\n",
      "Sister, content you in my discontent.\n",
      "Sir, to your pleasure humbly I subscribe:\n",
      "My books and instruments shall be my company,\n",
      "On them to took a town of mine eyes:\n",
      "Sleeping and waking, O, defend me still!\n",
      "\n",
      "Ghost of Prince Edward:\n",
      "\n",
      "Ghost of King Henry VI:\n",
      "\n",
      "Ghost of CLARENCE:\n",
      "\n",
      "Ghost of RIVERS:\n",
      "\n",
      "Ghost of GREY:\n",
      "\n",
      "Ghost of VAUGHAN:\n",
      "\n",
      "All:\n",
      "\n",
      "Ghost of HASTINGS:\n",
      "\n",
      "Ghosts of young Princes:\n",
      "\n",
      "Ghost of LADY ANNE:\n",
      "\n",
      "Ghost of BUCKINGHAM:\n",
      "\n",
      "KING RICHARD III:\n",
      "Give me another horse: bind up my wounds.\n",
      "Have mercy, Jesu!--Soft! I did but dream.\n",
      "O coward conscience, how dost thou afflict me!\n",
      "The lights burn blue. It is now dead midnight.\n",
      "Cold fearful drops stand on my trembling flesh.\n",
      "What do I fear?  myself?   there's none else by:\n",
      "Richard loves Richard; that is, I am I.\n",
      "Is there a murderer here?    No. Yes, I am:\n",
      "Then fly. What, from myself?   Great reason why:\n",
      "Lest I revenge. What, myself upon myself?\n",
      "Alack. I love myself. Wherefore? for any good\n",
      "That I myself have done unto myself?\n",
      "O, no! alas, I rather hate myself\n",
      "For hateful deeds committed by myself!\n",
      "I am a villain: yet I lie. I am not.\n",
      "Fool, of thyself speak well: fool, do not flatter.\n",
      "My conscience hath a thousand several tongues,\n",
      "And every tongue brings in a several tale,\n",
      "And every tale condemns me for a villain.\n",
      "Perjury, perjury, in the high'st degree\n",
      "Murder, stem murder, in the direst degree;\n",
      "All several sins, all used in each degree,\n",
      "Throng to the bar, crying all, Guilty! guilty!\n",
      "I shall despair. There is no creature loves me;\n",
      "And if I die, no soul shall pity me:\n",
      "Nay, wherefore should they, since that I myself\n",
      "Find in myself no pity to myself?\n",
      "Methought the sou\n"
     ]
    }
   ],
   "source": [
    "# alright, let's sample some character-level Shakespeare\n",
    "from mingpt.utils import sample\n",
    "\n",
    "context = \"O God, O God!\"\n",
    "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
    "y = sample(model, x, 2000, temperature=1.0, sample=True, top_k=10)[0]\n",
    "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# well that was fun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By @smth \n",
    "# If you also want to check cpuStats while do your GPU training, you can use psutil \n",
    "\n",
    "# prints currently alive Tensors and Variables\n",
    "# courtesy Smth, PyTorch Dev, Facebook AI Research\n",
    "# use the garbage collector’s book-keeping \n",
    "# to print out the currently resident Tensors. \n",
    "# Here’s a snippet that shows all the currently allocated Tensors:\n",
    "\n",
    "import sys, os, gc\n",
    "import psutil\n",
    "def memReport():\n",
    "    print(\"\\n\")\n",
    "    print(\"\\nmemReport\\n\")\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj):\n",
    "            print(type(obj), obj.size())\n",
    "    \n",
    "def cpuStats():\n",
    "        print(\"sys.version: \", sys.version)\n",
    "        #  cpu_percent() keeps track of CPU times since last call, and that's how it is able to determine percentage.\n",
    "        print(\"\\nCPU percent: \", psutil.cpu_percent())\n",
    "        print(\"\\nPhysical memory: \", psutil.virtual_memory())  # physical memory usage\n",
    "        pid = os.getpid() # this returns the current process\n",
    "        print(\"\\npid: \", pid)\n",
    "        py = psutil.Process(pid)\n",
    "        memoryUse = py.memory_info()[0] / 2. ** 30  # memory use in GB...I think\n",
    "        print('\\nmemory GB:', memoryUse)\n",
    "\n",
    "memReport()\n",
    "cpuStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Comments \n",
    "\n",
    "### * Memory management - Unlike many other languages, Python does not necessarily release the memory back to the Operating System. Instead, it has a dedicated object allocator for smaller objects.\n",
    "\n",
    "### * You will notice that Python created folders with __pycache__ and also files with .pyc or .pyo extension. These are bytecode-compiled and optimized bytecode-compiled versions of your program's files, respectively. They make your program start a little faster. When your scripts change, they will be recompiled, and if you delete the files or the whole folder and run your program again, they will reappear (unless you specifically suppress that behavior). Just put them in your gitignore\n",
    "\n",
    "### * Python garbage collection can also come in handy - but most of the time you don't need to use gc.collect but there some advantages in calling gs.collect at the end of your loop - you will reduce memory fragmentation. Trying to free the memory between computations using gc.collect() fixed the memory-related aspect of the problem but it could also result in performance issues.\n",
    "\n",
    "### * Of course Multi threaded programming might work much better instead of having to do gc.collect after the end of every loop. Once the thread ends, the memory is automatically freed without the strange performance issue. \n",
    "\n",
    "### * PyTorch uses a caching memory allocator to speed up memory allocations. This allows fast memory deallocation without device synchronizations. However, the unused memory managed by the allocator will still show as if used in nvidia-smi. \n",
    "#### * You can use memory_allocated() and max_memory_allocated() to monitor memory occupied by tensors, and use memory_cached() and max_memory_cached() to monitor memory managed by the caching allocator. \n",
    "#### * Calling empty_cache() releases all unused cached memory from PyTorch so that those can be used by other GPU applications. However, the occupied GPU memory by tensors will not be freed so it can not increase the amount of GPU memory available for PyTorch.\n",
    "\n",
    "### * a note about CPU time - Do be aware though that CPU percentage can jump around quite a lot from one moment to another and especially where the time period it is calculated over keeps changing. This can be quite confusing. Use a background thread which works out CPU percentage average over set time ranges.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
