# Gpu-training

This repo is built using minGPT (https://github.com/karpathy/minGPT). 
This is used a baseline to discuss some of the challenges of GPU training and how to be aware of memory issues and plan accordingly during your training phases of your NLP models. These memory suggestions are not limited to NLP - for any kind of AI problem. 

This is used as a demo file for Global AI/Big Data Conference presentation - on Sept 16th 2020 - to discuss some of the aspects of GPU training of NLP models using PyTorch.

Agenda
1. Background & Expectations
2. Quick overview of the GPU platform I am using for the demo - trainml.ai
3. Hyperparameters & Training
4. Options to check while the model is training
5. Memory management
6. Some experimental data
7. Final Thoughts